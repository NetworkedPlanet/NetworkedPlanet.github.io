<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NetworkedPlanet</title>
    <description>Experts in Linked and Open Data Publishing</description>
    <link>http://networkedplanet.com/</link>
    <atom:link href="http://networkedplanet.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 17 Feb 2016 10:30:54 +0000</pubDate>
    <lastBuildDate>Wed, 17 Feb 2016 10:30:54 +0000</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Linked Data 101 - What&#39;s a predicate?</title>
        <description>&lt;p class=&quot;well text-center&quot;&gt;This article is part of the &lt;a href=&quot;/blog/tags/ld101-series/&quot;&gt;Linked Data 101 series&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;identifiers-for-context&quot;&gt;Identifiers for context&lt;/h2&gt;

&lt;p&gt;Yesterday we took a look at &lt;a href=&quot;/blog/2016/02/16/linked-data-101-identifiers.html&quot;&gt;the use of URIs in linked data&lt;/a&gt;, we left off just as we were getting to a really interesting part - &lt;strong&gt;predicates&lt;/strong&gt;! &lt;/p&gt;

&lt;p&gt;When we look at two disparate sources of information on the web, how do we decide that the two pieces have overlapping meanings, even when they are talking about different subjects, how do we know they contain information relevant to each other? Currently we need to do that process manually, or have extremely complex linguistic programs to try and replicate that work for us. &lt;/p&gt;

&lt;p&gt;Let’s say I have some data that contains information about Wards, depending on the &lt;em&gt;context&lt;/em&gt; that information could be about rooms in a hospital, a division of a geographical area governed by councillors, or even a young person who has been appointed a guardian by a court. By using unique identifiers we have a way of making these differences in meaning explicit - and these identifiers are known as predicates.&lt;/p&gt;

&lt;p&gt;Ever since the initial vision of the World Wide Web was dreamt of, the idea of being able to embed context and meaning into that published information has been part of that plan. It was put on a back-burner for the initial versions of the web, but that goal has never been forgotten. With linked data (or as I like to think of it - &lt;em&gt;linkable&lt;/em&gt; data) we use a unique identifier to explicitly describe the link between a Thing and its properties, and what unique identifiers do we use on the web? You guessed it - &lt;a href=&quot;/blog/2016/02/16/linked-data-101-identifiers.html&quot;&gt;URIs&lt;/a&gt;. &lt;/p&gt;

&lt;p class=&quot;well pull-right lead&quot; style=&quot;width:320px; margin-left:1em;&quot;&gt;Many people see linked data as a subsequent step to open data - this could not be further from the truth!&lt;/p&gt;

&lt;p&gt;Given the way the levels of &lt;em&gt;open&lt;/em&gt; data are promoted, it’s unsurprising that many people see linked data as a subsequent step that can only be taken after you’ve made a choice about whether to publish your data publicly - &lt;strong&gt;this could not be further from the truth&lt;/strong&gt;! 
Rather than focus on how you plan to &lt;em&gt;publish&lt;/em&gt; your data - think of how you need to &lt;em&gt;use&lt;/em&gt; your data. Linked data makes it easier to consume external linked open datasets as well as merge and share data across departments even when staying private to your organisation, making it easier to spot trends and perform data analysis.&lt;/p&gt;

&lt;h2 id=&quot;building-your-vocabulary&quot;&gt;Building your vocabulary&lt;/h2&gt;

&lt;p style=&quot;clear:both;&quot;&gt;Now that we’re all caught up about WHY predicates, let’s look at the HOW. Here’s some simple CSV data and its presentation when structured as triples: &lt;/p&gt;

&lt;pre&gt;
ID,Event,Location - Lat,Location - Long,Date &amp;amp; Time
EV890,Introduction To Linked Data,53.3957985,-1.5694953,13/7/2016 11:00:00 AM
EV891,Query the Web of Data with SPARQL,53.2205365,-4.1601993,20/4/2016 7:00:00 PM
EV892,Better data means easier collaboration,55.8555367,-4.3024977,3/8/2016 6:30:00 PM
&lt;/pre&gt;

&lt;p class=&quot;alert alert-info&quot;&gt;For an explanation of the structure of a triple, please read the &lt;a href=&quot;/blog/2016/02/16/linked-data-101-identifiers.html&quot;&gt;first part in this Linked Data 101 series&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For my predicates I’ve chosen descriptive URIs much in the same way that I’d choose a table’s column headings.&lt;/p&gt;

&lt;pre&gt;
&amp;lt;http://example.org/events/EV890&amp;gt; &amp;lt;http://example.org/events/title&amp;gt; &quot;Introduction To Linked Data&quot; .
&amp;lt;http://example.org/events/EV890&amp;gt; &amp;lt;http://example.org/events/latitude&amp;gt; &quot;53.3957985&quot; .
&amp;lt;http://example.org/events/EV890&amp;gt; &amp;lt;http://example.org/events/longitude&amp;gt; &quot;-1.5694953&quot; .
&amp;lt;http://example.org/events/EV890&amp;gt; &amp;lt;http://example.org/events/date&amp;gt; &quot;2016-07-13T11:00:00.0000000+00:00&quot;^^&amp;lt;http://www.w3.org/2001/XMLSchema#dateTime&amp;gt; .
&amp;lt;http://example.org/events/EV891&amp;gt; &amp;lt;http://example.org/events/title&amp;gt; &quot;Query the Web of Data with SPARQL&quot; .
&amp;lt;http://example.org/events/EV891&amp;gt; &amp;lt;http://example.org/events/latitude&amp;gt; &quot;53.2205365&quot; .
&amp;lt;http://example.org/events/EV891&amp;gt; &amp;lt;http://example.org/events/longitude&amp;gt; &quot;-4.1601993&quot; .
&amp;lt;http://example.org/events/EV891&amp;gt; &amp;lt;http://example.org/events/date&amp;gt; &quot;2016-04-20T19:00:00.0000000+00:00&quot;^^&amp;lt;http://www.w3.org/2001/XMLSchema#dateTime&amp;gt; .
&amp;lt;http://example.org/events/EV892&amp;gt; &amp;lt;http://example.org/events/title&amp;gt; &quot;Better data means easier collaboration&quot; .
&amp;lt;http://example.org/events/EV892&amp;gt; &amp;lt;http://example.org/events/latitude&amp;gt; &quot;55.8555367&quot; .
&amp;lt;http://example.org/events/EV892&amp;gt; &amp;lt;http://example.org/events/longitude&amp;gt; &quot;-4.3024977&quot; .
&amp;lt;http://example.org/events/EV892&amp;gt; &amp;lt;http://example.org/events/date&amp;gt; &quot;2016-08-03T18:30:00.0000000+00:00&quot;^^&amp;lt;http://www.w3.org/2001/XMLSchema#dateTime&amp;gt; .
&lt;/pre&gt;

&lt;p&gt;As it is, the data would be fine to publish on your internal or external data portal (using your portal’s address rather than example.org of course!). It’s 4 star data, meaning that not only is it in a non-proprietary format but that you’ve used URIs to denote your resources. Your data is now &lt;strong&gt;&lt;em&gt;linkable&lt;/em&gt;&lt;/strong&gt;, regardless of whether you choose to share it outside of your organisation. &lt;/p&gt;

&lt;p&gt;Of course you want the set of predicates used in your data to be consistent across your organisation - and to do this you build a list of the predicates you use (along with some other useful terms) and put them in a &lt;strong&gt;vocabulary&lt;/strong&gt; which you can choose to keep private to your organisation, or - like the raw data itself - you can choose to publish these sets of terms to a wider audience.&lt;/p&gt;

&lt;h2 id=&quot;using-shared-vocabularies&quot;&gt;Using shared vocabularies&lt;/h2&gt;

&lt;p&gt;So how do we know that data from two different sources are talking about the same concepts? We do this by making use of &lt;strong&gt;shared vocabularies&lt;/strong&gt; - simply replace your predicate with the predicate from the published vocabulary. &lt;/p&gt;

&lt;p&gt;A few examples of published open vocabularies are &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://semanticweb.org/wiki/Dublin_Core&quot;&gt;Dublin Core vocabulary&lt;/a&gt;. This is a vocabulary published by the Dublin Core Metadata Initiative used to describe information about documents - such as Title, Description, Creator, and so on - so you can see why it would be such a well used vocabulary. &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.geonames.org/ontology/documentation.html&quot;&gt;Geonames&lt;/a&gt;. This contains properties for describing places such as Name, Alternative Name, Population and suchlike. It’s worthwhile to note that while Geonames has many properties for describing places, for latitude and longitude it reuses the &lt;a href=&quot;https://www.w3.org/2003/01/geo/&quot;&gt;W3C Basic Geo vocabulary&lt;/a&gt;. &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://schema.org&quot;&gt;Schema.org&lt;/a&gt;. A hugely popular vocabulary (due to its support in Google, Microsoft and Yahoo) is  Many plugins for CMS website use the terms defined in Schema.org in order to embed the information in their webpages that they are talking about (for example) dates of events, and therefore when they appear on the search result they have additional useful information about upcoming events under their site name and description.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://data.ordnancesurvey.co.uk/ontology&quot;&gt;Ordnance Survey ontologies&lt;/a&gt;. A set of vocabularies for describing administrative areas of Great Britain, spatial relations, and more. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’ve chosen to use the &lt;a href=&quot;https://www.w3.org/2003/01/geo/&quot;&gt;Basic Geo Vocab&lt;/a&gt; to describe my latitude and longitude, and &lt;a href=&quot;https://schema.org/Event&quot;&gt;Schema.org&lt;/a&gt; to describe the other details of my event. This changes my RDF to:&lt;/p&gt;

&lt;pre&gt;
&amp;lt;http://example.org/events/EV890&amp;gt; &amp;lt;http://schema.org/name&amp;gt; &quot;Introduction To Linked Data&quot; .
&amp;lt;http://example.org/events/EV890&amp;gt; &amp;lt;http://www.w3.org/2003/01/geo/wgs84_pos#lat&amp;gt; &quot;53.3957985&quot; .
&amp;lt;http://example.org/events/EV890&amp;gt; &amp;lt;http://www.w3.org/2003/01/geo/wgs84_pos#long&amp;gt; &quot;-1.5694953&quot; .
&amp;lt;http://example.org/events/EV890&amp;gt; &amp;lt;http://schema.org/startDate&amp;gt; &quot;2016-07-13T11:00:00.0000000+00:00&quot;^^&amp;lt;http://www.w3.org/2001/XMLSchema#dateTime&amp;gt; .
&amp;lt;http://example.org/events/EV891&amp;gt; &amp;lt;http://schema.org/name&amp;gt; &quot;Query the Web of Data with SPARQL&quot; .
&amp;lt;http://example.org/events/EV891&amp;gt; &amp;lt;http://www.w3.org/2003/01/geo/wgs84_pos#lat&amp;gt; &quot;53.2205365&quot; .
&amp;lt;http://example.org/events/EV891&amp;gt; &amp;lt;http://www.w3.org/2003/01/geo/wgs84_pos#long&amp;gt; &quot;-4.1601993&quot; .
&amp;lt;http://example.org/events/EV891&amp;gt; &amp;lt;http://schema.org/startDate&amp;gt; &quot;2016-04-20T19:00:00.0000000+00:00&quot;^^&amp;lt;http://www.w3.org/2001/XMLSchema#dateTime&amp;gt; .
&amp;lt;http://example.org/events/EV892&amp;gt; &amp;lt;http://schema.org/name&amp;gt; &quot;Better data means easier collaboration&quot; .
&amp;lt;http://example.org/events/EV892&amp;gt; &amp;lt;http://www.w3.org/2003/01/geo/wgs84_pos#lat&amp;gt; &quot;55.8555367&quot; .
&amp;lt;http://example.org/events/EV892&amp;gt; &amp;lt;http://www.w3.org/2003/01/geo/wgs84_pos#long&amp;gt; &quot;-4.3024977&quot; .
&amp;lt;http://example.org/events/EV892&amp;gt; &amp;lt;http://schema.org/startDate&amp;gt; &quot;2016-08-03T18:30:00.0000000+00:00&quot;^^&amp;lt;http://www.w3.org/2001/XMLSchema#dateTime&amp;gt; .
&lt;/pre&gt;

&lt;p&gt;Now when I merge my data with other data sources using the same vocabulary, our events and locations merge with ease.&lt;/p&gt;

&lt;h2 id=&quot;where-to-start&quot;&gt;Where to start?&lt;/h2&gt;

&lt;p&gt;It’s all very well for me to talk about how I chose to use a couple of popular vocabularies and then plonked them in my data, but how are you going to know about these vocabularies in the first place, and what should influence your decision on which to use? Fear not, there are websites out there that help you find existing published vocabularies, such as &lt;a href=&quot;http://lov.okfn.org/dataset/lov&quot;&gt;LOV&lt;/a&gt;, you can reach out to the publishers of open data that you are consuming (such as local or national government) and ask them about their preferred vocabularies, and of course we would also suggest &lt;a href=&quot;http://networkedplanet.com/#contact&quot;&gt;getting in touch with your friendly linked data experts&lt;/a&gt; to ask for advice too (yes, that is indeed a link to our contact detail page, it had to be done!). &lt;/p&gt;

&lt;p&gt;Can’t find a shared vocabulary that matches your needs? Consider taking your private vocabulary and publishing it as open data. That’s exactly what the Ordnance Survey chose to do, why wait for someone else to do it when you can lead the field?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I hope you’ve found this article informative! Please feel free to use the comments section with any feedback or questions. Next up in the series we’re going to take what we’ve learnt about identifiers and triples and use our &lt;a href=&quot;http://levelup.networkedplanet.com&quot;&gt;Level Up Csv to RDF converter&lt;/a&gt; to grab some boring flat data and give it a semantic boost. &lt;/p&gt;

&lt;p class=&quot;well text-center&quot;&gt;This article is part of the &lt;a href=&quot;/blog/tags/ld101-series/&quot;&gt;Linked Data 101 series&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 17 Feb 2016 00:00:00 +0000</pubDate>
        <link>http://networkedplanet.com/blog/2016/02/17/linked-data-101-predicates.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2016/02/17/linked-data-101-predicates.html</guid>
        
        <category>LD101 series</category>
        
        <category>linked data</category>
        
        <category>predicates</category>
        
        <category>vocabularies</category>
        
        <category>ontologies</category>
        
        
      </item>
    
      <item>
        <title>Linked Data 101 - What&#39;s with all the URIs?</title>
        <description>&lt;p class=&quot;well text-center&quot;&gt;This article is part of the &lt;a href=&quot;/blog/tags/ld101-series/&quot;&gt;Linked Data 101 series&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ur-whatnow-&quot;&gt;UR-WhatNow ?&lt;/h2&gt;

&lt;p&gt;URI is an acronym for &lt;strong&gt;Uniform Resource Identifier&lt;/strong&gt;, which is one of those whole names that tells you nothing at all. A URI is simply a particular way of writing a name for a thing. A common kind of URI is the URL (Uniform Resource &lt;strong&gt;Locator&lt;/strong&gt;) where that name is actually treated as an address and provides you with a way to look up the identified thing. The address of a webpage like this one is a URL.&lt;/p&gt;

&lt;p&gt;People are so used to seeing tabular data with numerical IDs, and so used to only ever seeing URIs as “things you click on to go to a website” - that the use of URIs as data record identifiers can be a stumbling block for a lot of people when we talk about linked data. Don’t worry, you’re not alone! Let’s just take baby steps through the idea of using URIs, and by the end of the post you shouldn’t still have that nagging feeling of confusion when you see URIs all over the place in linked data.&lt;/p&gt;

&lt;h2 id=&quot;the-identifier&quot;&gt;The identifier&lt;/h2&gt;

&lt;p&gt;Each record in your data needs some way to identify it, we might be used to seeing this as a numerical ID or a more complex code, but we all are comfortable with the idea that in order to group all the values that attach to a record (e.g. title, description, location) we need a way to identify that record as a “thing”. 
In relational databases, the ID of one table (such as products) has a “primary key” that is then referred to as a foreign key from another table, thereby linking the two records. &lt;/p&gt;

&lt;p&gt;It makes sense then, that in the far wider Web of Data, using URIs as those primary and foreign keys makes complete sense. Arbitrary codes or numbers have no way of containing the information needed to locate that data on the web. In particular it makes a whole load of sense to use HTTP URIs for a couple of reasons. Firstly HTTP URIs form a neat, globally managed system where you can buy a domain and then manage the content of that domain as you see fit. As long as everyone sticks to the rule of only creating new URIs in domains that they control you can happily make new URIs under the domains that you control without having to worry about name clashes. Secondly HTTP is also a &lt;strong&gt;protocol&lt;/strong&gt; - it defines a way in which an application can take the HTTP identifier and use it to request some information from a server - and this is where the Web part of the Web of Data happens.&lt;/p&gt;

&lt;h2 id=&quot;im-just-making-up-web-addresses&quot;&gt;404 - I’m just making up web addresses!&lt;/h2&gt;

&lt;p&gt;In most tutorials and walkthroughs you’ll find people using identifiers such as &lt;code&gt;http://data.example.org/category/pets&lt;/code&gt; or similar. Yep, they don’t “go anywhere” if you put them into your web browser. In a sense, that is perfectly fine, the URI is an identifier not an address.&lt;/p&gt;

&lt;p&gt;However, if your goal is to populate a Linked Data browser on an open data portal then of course you’ll need to use your data portal’s address as the root domain, the rest of the URI depends on how you’d like to present your resources to the outside world. If you &lt;em&gt;really&lt;/em&gt; want you could present every resource you keep &lt;code&gt;http://yourdataportal/1&lt;/code&gt;, &lt;code&gt;http://yourdataportal/2&lt;/code&gt;, &lt;code&gt;http://yourdataportal/3&lt;/code&gt;, &lt;code&gt;http://yourdataportal/4&lt;/code&gt; etc, but I strongly advise against it! You’ll find it a lot easier to navigate and work with your data using URIs that are fairly easy to read by humans as well as computers.&lt;/p&gt;

&lt;h2 id=&quot;uris-are-everywhere&quot;&gt;URIs are everywhere&lt;/h2&gt;

&lt;p&gt;You’ll have seen that the RDF is made up of triples:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Subject&lt;/th&gt;
      &lt;th&gt;Predicate&lt;/th&gt;
      &lt;th&gt;Object&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The identifier&lt;/td&gt;
      &lt;td&gt;The link between the identifier and some property of the resource&lt;/td&gt;
      &lt;td&gt;The value of the property&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Must be a URI&lt;/td&gt;
      &lt;td&gt;Must be a URI&lt;/td&gt;
      &lt;td&gt;Can be a literal or a URI&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Think of how you read information out of an Excel spreadsheet. For example, if I want to find the stock level of a product, I look up the product, read along the column headers until I see the column for stock level, then read the relevant value out of that column for the product’s record.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/linkeddata101/csv-to-predicates.png&quot; alt=&quot;reading triples from tabular data&quot; class=&quot;img-medium img-responsive center-block bordered-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example the triple would look something like:&lt;/p&gt;

&lt;pre&gt;
&amp;lt;http://data.example.org/products/LD12955AD&amp;gt; &amp;lt;http://data.example.org/stockLevel&amp;gt; &quot;110&quot;^^&amp;lt;http://www.w3.org/2001/XMLSchema#integer&amp;gt;
&lt;/pre&gt;

&lt;p&gt;The way we are writing triples here, the three parts are separated from each other by spaces and angle-brackets (&amp;lt;&amp;gt;) are used to wrap URI identifiers. You will also see that the value at the end of the line (“249”) is followed by a double-caret (^^) and another URI - this last URI is specifying the &lt;strong&gt;data type&lt;/strong&gt; of the value. In effect saying that the string “249” should be processed as an integer. This is an example of a triple with a literal (in this case the literal is the integer value 249). We can also create triples that have a URI as the object:&lt;/p&gt;

&lt;pre&gt;
&amp;lt;http://data.example.org/products/LD12955AD&amp;gt; &amp;lt;http://data.example.org/manufacturer&amp;gt; &amp;lt;http://contoso.com/#company&amp;gt;
&lt;/pre&gt;

&lt;p&gt;We can see why URIs must be used as subjects, and &lt;em&gt;can&lt;/em&gt; be objects, what about the predicates? Why not just use text labels to describe properties of a resource such as “stock level” or “manufacturer”? Well, without using URIs we would miss out on the core power of the Web of Data - a way of being able to embed our data with the information that we are talking about the same &lt;em&gt;concepts&lt;/em&gt; that another data publisher is describing in &lt;em&gt;their&lt;/em&gt; data. &lt;/p&gt;

&lt;p&gt;That’s exactly what the &lt;a href=&quot;/blog/2016/02/17/linked-data-101-predicates.html&quot;&gt;second blog post&lt;/a&gt; in this series is going to concentrate on - predicates and vocabularies - stay tuned!&lt;/p&gt;

&lt;p class=&quot;well text-center&quot;&gt;This article is part of the &lt;a href=&quot;/blog/tags/ld101-series/&quot;&gt;Linked Data 101 series&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Feb 2016 00:00:00 +0000</pubDate>
        <link>http://networkedplanet.com/blog/2016/02/16/linked-data-101-identifiers.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2016/02/16/linked-data-101-identifiers.html</guid>
        
        <category>LD101 series</category>
        
        <category>linked data</category>
        
        <category>identifiers</category>
        
        
      </item>
    
      <item>
        <title>Level Up Your Data!</title>
        <description>&lt;p&gt;There is a lot of buzz around the term “open data” at the moment, and rightly so! As more and more organisations open up their data, it builds a data-ecosystem of creative opportunities that are simply not possible when said data is locked away in private silos. There are &lt;a href=&quot;http://www.opendataenterprise.org/map/viz/index.html&quot;&gt;more and more cases being highlighted&lt;/a&gt; that show the power of open data, even more fantastic when you realise that the stories you hear about will only ever be a fraction of what is really using open data - data, by its very nature, is a behind-the-scenes kind of deal, and you most likely make use of it every data without being aware of it. &lt;/p&gt;

&lt;p&gt;If you’ve been reading about open data you’ve most likely heard of &lt;a href=&quot;http://5stardata.info/en/&quot;&gt;Sir Tim Berners-Lee’s deployment scheme for open data&lt;/a&gt;. In short, &lt;em&gt;any&lt;/em&gt; data you release on the web under an open license is open data - anything! But it’s easy to see that whilst a PDF report released as open data might be useful for someone to read, it’s going to make someone’s job a LOT tougher if they want to build on the data contained within it in any useful way. &lt;/p&gt;

&lt;p&gt;The differences between the subsequent levels follows exactly the same reasoning - an excel spreadsheet is a more structured way of presenting the data of the PDF report, which makes it more re-usable by the people who come to reuse it, but we’re still dealing with a propriatary format. The obvious next step is to convert that data to a non-propriatary format of structured data like CSV. Now you raw data on the web in a format that everyone can use - and we arrive at 3 star open data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/levelup/5-star-steps.png&quot; alt=&quot;5 star open data&quot; class=&quot;img-medium img-responsive center-block bordered-image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hang on, doesn’t this show 5 levels of open data?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Why yes! Yes it does! In the same way that CSV is by far an easier format to work with data than PDF, linked open data is another two steps “easier” to work with by formatting it as semantic data rather. What do I mean by semantic data? Well, instead of having tables and rows of values (that require a human to “know” what the table represents), we instead structure that data in a way that demonstrates that each of those records is a “Thing”; that Thing has a bunch of properties (such as length, name, colour, author - anything) and we follow the convention of each Thing having an identifier that is a URI. When you structure your data in this way it is luscious 4 star data, and if you choose to publish it under an open license and link those Things to other Things - wooo! 5 star open data!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You lost me at “semantic”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ah yes, well, you’re not alone! Many people find it a difficult concept to grasp, I suspect this is one of the major reasons why so many organisations that publish open data find themselves stuck at the 3-star level. Whilst it can &lt;em&gt;feel&lt;/em&gt; like a big difference, the actual work taken to Level Up from 3-star to 4-star (and thereby making it possible to reach 5-star) is actually very small. By far the largest chunk of work in &lt;em&gt;any&lt;/em&gt; data publishing process is to clean the initial data and structure it well so that it can be published - this must be done even with CSV files (no one thanks you for “data fly tipping”). The bulk of the work is already done - a very small step is all that’s needed to take that CSV and make it delicious, addressable, RDF goodness.&lt;/p&gt;

&lt;p&gt;Here at NetworkedPlanet we’ve been working with companies to semantically empower their data for over ten years, so we have learnt a few things along the way! One of which is that if you can see a way of future-proofing your tech development, and it’s within your means to do so, then do it! After chatting with people about linked data a pattern emerged and I noticed that people were getting bogged down in the psychology of semantic information architecture, and missed how simple the difference between CSV and RDF really is. &lt;a href=&quot;http://levelup.networkedplanet.com&quot;&gt;So we wrote an online converter to show you all how to get going&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/levelup/start_sm.png&quot; alt=&quot;Drag and drop CSV files to start the Level Up process&quot; class=&quot;img-medium img-responsive center-block bordered-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s super simple - just drag and drop a CSV file (make note of the filesize limits!) or click to select a file for conversion. This uploads the file to our server (do read the terms and conditions!) and loads up the mapping page. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/levelup/map_sm.png&quot; alt=&quot;Map the column headings to RDF predicates&quot; class=&quot;img-medium img-responsive center-block bordered-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This consists of showing you each of your CSV columns, with controls where you can select what data-types the column values are, or modify the URIs used as predicates, and so on. Don’t worry if you’re not sure what predicates are - we have webinars and blog posts about that coming up if you want to understand linked open data. For your first go, just leave all the defaults in, hit Convert, and download the resulting zip file that contains your 4-star data in different formats.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/levelup/success_sm.png&quot; alt=&quot;Done! Download your converted files as a zip of XML-RDF, N-Triples and Turtle files&quot; class=&quot;img-medium img-responsive center-block bordered-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From there you can have a play around with the converter to make your output even more pretty. Got some latitude and longitude values in your columns? Why not make use of the &lt;a href=&quot;https://www.w3.org/2003/01/geo/&quot;&gt;W3C Basic Geo vocab&lt;/a&gt; instead of the automagically created predicates? Set your datatypes so that numbers, dates and text in particular languages are specified as such in the output. Got a code for a resource over on another linked data portal? Use a URI template to turn the code into a valid link to that item - this is not just data &lt;strong&gt;on&lt;/strong&gt; the web, this is data &lt;strong&gt;in&lt;/strong&gt; the web.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;
&lt;a class=&quot;btn-lg btn-primary&quot; href=&quot;http://levelup.networkedplanet.com&quot;&gt;Level Up your data now!&lt;/a&gt;
&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Feb 2016 00:00:00 +0000</pubDate>
        <link>http://networkedplanet.com/blog/2016/02/15/level-up-your-data.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2016/02/15/level-up-your-data.html</guid>
        
        <category>csv</category>
        
        <category>rdf</category>
        
        <category>convert</category>
        
        <category>linked data</category>
        
        
      </item>
    
      <item>
        <title>Don&#39;t Forget Metadata!</title>
        <description>&lt;p&gt;I have many fond memories as a kid of our local libraries. I was fortunate to grow up a short walk away from
what was at the time the &lt;a href=&quot;https://en.wikipedia.org/wiki/Birmingham_Central_Library&quot;&gt;largest non-national library in Europe&lt;/a&gt; 
as well as a closer local library in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spring_Hill_Library&quot;&gt;grand
red-brick Victorian building&lt;/a&gt;. 
One of those memories is the card indexes. I loved the card index (yes,
I was a metadata geek from an early age). Armed only with a question and some idea about keywords to 
use you could riffle through wooden drawers full of pink, blue and beige cards and then wander off 
to the stacks to find the answers. Magic stuff.&lt;/p&gt;

&lt;p&gt;How does this translate when we are talking about data and datasets? Well, there are a number of existing 
RDF ontologies that touch on the area of metadata, but probably the
most well-known and widely used one for describing dataset metadata is &lt;a href=&quot;https://www.w3.org/TR/void/&quot;&gt;VoID&lt;/a&gt;.
VoID provides you with the means to identify and describe datasets as well as to provide metadata
ranging from contact and licensing information to example dataset resource URIs and patterns for accessing
the content of the dataset.&lt;/p&gt;

&lt;p&gt;The question is where to place this metadata? One obvious answer is to add the metadata in to 
the RDF dataset that it describes. This is great for the user that has chosen to access that data; 
but in this loose book/dataset analogy, metadata that is contained inside your dataset is equivalent to 
that US Library of Congress metadata that you find inside the front cover of a book. &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dataset metadata should be the card index to all the open data you publish. 
Without it being in a single central location, your users must aimlessly wander the shelves, pulling down dataset after 
dataset in search of what they want.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A better solution is to treat the metadata about datasets as a separate dataset in its own right. 
Think of this as the downloadable / queryable card-index to your data library. Indeed the VoID recommendation
goes as far as to &lt;a href=&quot;https://www.w3.org/TR/void/#discovery&quot;&gt;suggest&lt;/a&gt; that you use a “well-known” URL for the location of that dataset - sort of
the equivalent of having your virtual card-index always just on the left as you come through the door (that was 
the sound of an analogy getting pushed to breaking point).&lt;/p&gt;

&lt;p&gt;Metadata management doesn’t have to be hard. Our &lt;a href=&quot;http://dataplatform.co.uk/&quot;&gt;Data Platform&lt;/a&gt; product has VoID 
metadata management built-in so when you update the dataset the metadata stays in sync. But regardless of what 
method you use to publish open data, it is important to recognize that your metadata is one of the key components
to making your data discoverable.&lt;/p&gt;

&lt;p&gt;Image by &lt;a href=&quot;http://www.flickr.com/photos/mamsy/&quot;&gt;http://www.flickr.com/photos/mamsy/&lt;/a&gt;, &lt;a href=&quot;http://creativecommons.org/licenses/by/2.0&quot;&gt;CC BY 2.0&lt;/a&gt;, via Wikimedia Commons&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Jan 2016 00:00:00 +0000</pubDate>
        <link>http://networkedplanet.com/blog/2016/01/25/dont-forget-metadata.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2016/01/25/dont-forget-metadata.html</guid>
        
        <category>open data</category>
        
        <category>metadata</category>
        
        <category>datasets</category>
        
        <category>VoID</category>
        
        
      </item>
    
      <item>
        <title>Newsletter Issue 2 out now!</title>
        <description>&lt;p&gt;The second issue of our email newsletter has hit the &lt;a href=&quot;http://networkedplanet.us11.list-manage.com/subscribe?u=62daac7ae19a90ef03ea3bd42&amp;amp;id=a797a2dd15&quot;&gt;mailing list&lt;/a&gt; inboxes. &lt;/p&gt;

&lt;p&gt;We chat about our new short animation about Linked Open Data, our 3-part free webinar series where you can learn all about the fundamental technologies of Linked Data, our upcoming online CSV converter - “Level Up”, new product releases and interesting reports from the wider Web of Linked and Open Data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://networkedplanet.us11.list-manage.com/subscribe?u=62daac7ae19a90ef03ea3bd42&amp;amp;id=a797a2dd15&quot;&gt;Sign up to our mailing list&lt;/a&gt; - we send newsletters roughly quarterly, possibly punctuated with the odd single-shot news item if we think it’s important!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://us11.campaign-archive2.com/?u=62daac7ae19a90ef03ea3bd42&amp;amp;id=a78b245052&amp;amp;e=&quot;&gt;Click here to read this issue&lt;/a&gt; in your browser, and &lt;a href=&quot;http://us11.campaign-archive1.com/home/?u=62daac7ae19a90ef03ea3bd42&amp;amp;id=a797a2dd15&quot;&gt;access the newsletter archive here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 19 Jan 2016 00:00:00 +0000</pubDate>
        <link>http://networkedplanet.com/blog/2016/01/19/newsletter-issue-2.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2016/01/19/newsletter-issue-2.html</guid>
        
        <category>newsletter</category>
        
        <category>mailing list</category>
        
        
      </item>
    
      <item>
        <title>NetworkedPlanet Webinars</title>
        <description>&lt;p&gt;We are pleased to announce our second series of training webinars. Following on from the couple of successful webinars we presented
last year, Season 2 of the NetworkedPlanet webinars is now three episodes long.&lt;/p&gt;

&lt;div vocab=&quot;http://schema.org/&quot;&gt;
&lt;div typeof=&quot;EducationEvent&quot;&gt;
&lt;span property=&quot;description&quot;&gt;
In the &lt;a href=&quot;https://www.eventbrite.co.uk/e/webinar-introduction-to-rdf-and-sparql-tickets-20856883500&quot; onclick=&quot;trackOutboundLink(&#39;https://www.eventbrite.co.uk/e/webinar-introduction-to-rdf-and-sparql-tickets-20856883500&#39;); return false;&quot; property=&quot;url&quot;&gt;&lt;span property=&quot;name&quot;&gt;Introduction to RDF and SPARQL&lt;/span&gt;&lt;/a&gt;
(&lt;meta property=&quot;startDate&quot; content=&quot;2016-02-04T14:00:00Z&quot; /&gt;4th Feb, &lt;meta property=&quot;duration&quot; content=&quot;1H&quot; /&gt;14:00-15:00 GMT) you will learn the technical fundamentals of linked open data. 
This presentation focusses on the technical details of what exactly RDF is and how the SPARQL language allows you to query RDF data.
&lt;/span&gt;
&lt;/div&gt;

&lt;a class=&quot;btn btn-primary&quot; href=&quot;https://www.eventbrite.co.uk/e/webinar-introduction-to-rdf-and-sparql-tickets-20856883500&quot; onclick=&quot;trackOutboundLink(&#39;https://www.eventbrite.co.uk/e/webinar-introduction-to-rdf-and-sparql-tickets-20856883500&#39;); return false;&quot;&gt;Sign Up For RDF/SPARQL&lt;/a&gt;
&lt;p&gt;&lt;/p&gt;

&lt;div typeof=&quot;EducationEvent&quot;&gt;
&lt;span property=&quot;description&quot;&gt;
The &lt;a href=&quot;https://www.eventbrite.co.uk/e/webinar-introduction-to-linked-data-tickets-20857152304&quot; onclick=&quot;trackOutboundLink(&#39;https://www.eventbrite.co.uk/e/webinar-introduction-to-linked-data-tickets-20857152304&#39;); return false;&quot; property=&quot;url&quot;&gt;&lt;span property=&quot;name&quot;&gt;Introduction to Linked Data&lt;/span&gt;&lt;/a&gt;
(&lt;meta property=&quot;startDate&quot; content=&quot;2016-02-18T14:00:00Z&quot; /&gt;18th Feb, &lt;meta property=&quot;duration&quot; content=&quot;1H&quot; /&gt;14:00-15:00 GMT) is a (mostly) non-technical presentation focussed on the what and why of Linked Data - what is Linked Data, what are 
the benefits of publishing and using Linked Data, what are the steps to take in publishing your own
Linked Data either internally or externally.
&lt;/span&gt;
&lt;/div&gt;

&lt;a class=&quot;btn btn-primary&quot; href=&quot;https://www.eventbrite.co.uk/e/webinar-introduction-to-linked-data-tickets-20857152304&quot; onclick=&quot;trackOutboundLink(&#39;https://www.eventbrite.co.uk/e/webinar-introduction-to-linked-data-tickets-20857152304&#39;); return false;&quot;&gt;Sign Up For Linked Data&lt;/a&gt;
&lt;p&gt;&lt;/p&gt;

&lt;div typeof=&quot;EducationEvent&quot;&gt;
&lt;span property=&quot;description&quot;&gt;
The &lt;a href=&quot;https://www.eventbrite.co.uk/e/webinar-sparql-101-tickets-20857308772&quot; onclick=&quot;trackOutboundLink(&#39;https://www.eventbrite.co.uk/e/webinar-sparql-101-tickets-20857308772&#39;); return false;&quot; property=&quot;url&quot;&gt;&lt;span property=&quot;name&quot;&gt;SPARQL 101&lt;/span&gt;&lt;/a&gt; webinar
(&lt;meta property=&quot;startDate&quot; content=&quot;2016-03-10T14:00:00Z&quot; /&gt;10th March, &lt;meta property=&quot;duration&quot; content=&quot;1H&quot; /&gt;14:00-15:00 GMT) focusses on the SPARQL
query language for RDF. Building on what was learnt in the introduction webinar, this webinar goes into more detail about the query
language and shows some examples of how to use SPARQL to explore datasets, extract salient information from them and create reports
by aggregating data.
&lt;/span&gt;
&lt;/div&gt;

&lt;a class=&quot;btn btn-primary&quot; href=&quot;https://www.eventbrite.co.uk/e/webinar-sparql-101-tickets-20857308772&quot; onclick=&quot;trackOutboundLink(&#39;https://www.eventbrite.co.uk/e/webinar-sparql-101-tickets-20857308772&#39;); return false;&quot;&gt;Sign Up For SPARQL 101&lt;/a&gt;
&lt;p&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;All of these webinars will be an hour long, with about 45-50 minutes of presentation followed by some time for questions afterwards.
The webinars will be presented by Kal Ahmed who in addition to being the founder of NetworkedPlanet is also the lead developer of &lt;a href=&quot;http://brightstardb.com/&quot;&gt;BrightstarDB&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These webinars are FREE to attend, but places are limited - so be sure to register using the links above to assure your place!&lt;/p&gt;

&lt;p&gt;We are already planning Season 3 with possible additional episodes covering things such as &lt;a href=&quot;http://brightstardb.com/&quot;&gt;BrightstarDB&lt;/a&gt;;
and the &lt;a href=&quot;http://dataplatform.co.uk/&quot;&gt;Data Platform&lt;/a&gt;. If you would like to get a heads-up on these
future webinars when they are announced, &lt;a href=&quot;https://twitter.com/nwplanet&quot;&gt;follow us on Twitter&lt;/a&gt; or &lt;a href=&quot;/#contact&quot;&gt;sign-up for the NetworkedPlanet Newsletter&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Jan 2016 00:00:00 +0000</pubDate>
        <link>http://networkedplanet.com/blog/2016/01/18/linked-data-webinars-season-2.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2016/01/18/linked-data-webinars-season-2.html</guid>
        
        <category>webinar</category>
        
        <category>rdf</category>
        
        <category>sparql</category>
        
        <category>linked data</category>
        
        
      </item>
    
      <item>
        <title>Closed / Shared / Open</title>
        <description>&lt;p&gt;While we are great fans of Open Data, it is just one part of a spectrum from data that
needs to be kept secure (such as personal information) through data that is shared between
partner organisations all the way to data that is made available to all to use and reuse.&lt;/p&gt;

&lt;p&gt;This nice, short video from the ODI explains the concepts of Closed, Shared and Open data. &lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/125783029&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/125783029&quot;&gt;Open / Shared / Closed: The world of data&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/theodiuk&quot;&gt;Open Data Institute&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Jan 2016 00:00:00 +0000</pubDate>
        <link>http://networkedplanet.com/blog/2016/01/18/closed-shared-open.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2016/01/18/closed-shared-open.html</guid>
        
        <category>odi</category>
        
        <category>open data</category>
        
        <category>video</category>
        
        
      </item>
    
      <item>
        <title>SPARQL CONSTRUCT 101</title>
        <description>&lt;p&gt;A SPARQL query can be written to return one of three things - SELECT returns a tabular set of results, ASK returns a simple boolean (true or false), and CONSTRUCT returns an RDF graph. For most human interaction with SPARQL endpoints we tend to prefer tables as they are just easier for us to grok, but if you are working with code that has an RDF parser available to it, then CONSTRUCT can really be your best friend and it is worth spending some time to understand how CONSTRUCT works and what cool sorts of things you can do with it.&lt;/p&gt;

&lt;p&gt;I’m going to walk through a few features of SPARQL CONSTRUCT using a really simple made-up sample file. If you would like to follow along, you can get the sample file I’m using from &lt;a href=&quot;https://gist.github.com/kal/ee1260ceb462d8e0d5bb&quot;&gt;this gist on GitHub&lt;/a&gt;. You will need a triple-store to query too - if you don’t have one installed (and if you are on Windows), why not try out &lt;a href=&quot;http://brightstardb.codeplex.com/&quot;&gt;BrightstarDB&lt;/a&gt;? BrightstarDB comes with a management application called Polaris which makes it easy to create a new store, import the file and run SPARQL queries. You can read about Polaris &lt;a href=&quot;http://brightstardb.readthedocs.org/en/latest/Using_Polaris/&quot;&gt;here in the BrightstarDB documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The sample dataset I’ve produced is really simple, it lists 13 imaginary server transaction logs. Each transaction log records the server that processed the transaction, the date/time that the transaction was processed and the status code of the transaction (just for fun there are some failed transactions in there).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Lets start with a really simple query to just list out the transactions in a table. For a table result we use a SELECT query::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PREFIX log: &amp;lt;http://example.org/ont/transaction-log/&amp;gt;
SELECT ?txn WHERE { ?txn a log:Transaction . }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SPARQL is based on matching patterns in the graph, so this simple query finds all triples
that match a pattern &lt;code&gt;{something} rdf:type log:Transaction&lt;/code&gt; (&lt;code&gt;a&lt;/code&gt; is just short-hand for rdf:type, you will see it is also used in the sample file). Then all of the values that match
{something} are assigned (in SPARQL-parlance &lt;strong&gt;bound&lt;/strong&gt;) to the variable ?txt. Each match found
results in a separate row in our results table. As this sample file contains 13 transaction logs,
the query will return a table with a single column and 13 rows like this:&lt;/p&gt;

&lt;table class=&quot;table table-striped&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;?txn&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/134&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/135&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/130&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/131&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/132&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/133&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/129&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/124&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/127&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/126&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/123&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can see that the results are not in the same order that they were specified in the source file, or even in order of their value - to order our results we would need to explicitly specify that in our query.&lt;/p&gt;

&lt;p&gt;Let’s suppose our challenge is to list transactions grouped together by the server that they are processed on. First we need to add the server itself to our triple pattern in the query:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PREFIX log: &amp;lt;http://example.org/ont/transaction-log/&amp;gt;
SELECT * WHERE { 
    ?txn a log:Transaction . 
    ?txn log:processedBy ?server
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I’m still using a &lt;code&gt;SELECT&lt;/code&gt; at the moment so that you can see more easily what is going on, but notice that I’ve changed the bit after the &lt;code&gt;SELECT&lt;/code&gt; to be &lt;code&gt;*&lt;/code&gt;. &lt;code&gt;SELECT *&lt;/code&gt; simply selects all of the variables in the pattern part of the query. Often in a query you will have variables in the pattern that are not useful in the results; but when exploring data and when debugging queries that are not returning the expected results I often switch from &lt;code&gt;SELECT ?v1 ?v2&lt;/code&gt; to &lt;code&gt;SELECT *&lt;/code&gt; so that I can see all of the variables.&lt;/p&gt;

&lt;p&gt;Now we have two columns in our results:&lt;/p&gt;

&lt;table class=&quot;table table-striped&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;?txn&lt;/th&gt;
      &lt;th&gt;?server&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/130&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/135&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/134&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/129&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/131&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/128&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/125&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/124&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/127&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/126&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/123&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/132&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/133&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/B&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If we were sticking with SELECT I would go for using a SORT now to sort by server which effectively gives us a form of grouping:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PREFIX log: &amp;lt;http://example.org/ont/transaction-log/&amp;gt;
SELECT * WHERE { 
    ?txn a log:Transaction . 
    ?txn log:processedBy ?server
} ORDER BY ?server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Resulting in:&lt;/p&gt;

&lt;table class=&quot;table table-striped&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;?txn&lt;/th&gt;
      &lt;th&gt;?server&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/135&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/132&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/129&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/126&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/123&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/130&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/133&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/124&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/127&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/134&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/131&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/128&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/125&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/C&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To process this sort of result I would still need to do some basic grouping work on the client - I would loop through the rows checking when the &lt;code&gt;?server&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;But if my client code can handle RDF then I can instead use a CONSTRUCT with these results to build a little RDF graph that I can process. CONSTRUCT works by defining a second pattern 
that looks a lot like the pattern in the WHERE part of the query. Here is a simple example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PREFIX log: &amp;lt;http://example.org/ont/transaction-log/&amp;gt;
CONSTRUCT {
    ?server log:processed ?txn .
}
WHERE { 
    ?txn a log:Transaction . 
    ?txn log:processedBy ?server
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The best way to visualize how CONSTRUCT works is to imagine that the SPARQL query processor first executes the WHERE part with a SELECT * so that it has a table of all the matches and access to all of the variables you used in the WHERE pattern. Then it loops through those rows one at a time and it generates triples by replacing the value found for each variable into the pattern you have provided in the CONSTRUCT.
 So when processing this sample query above it might find that the first row of the results table is:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;?txn&lt;/th&gt;
      &lt;th&gt;?server&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;http://example.org/data/transaction/135&lt;/td&gt;
      &lt;td&gt;http://example.org/data/server/A&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Resulting in the generation of a single triple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;http://example.org/data/server/A&amp;gt; log:processed &amp;lt;http://example.org/data/transaction/135&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here is our first cool thing about CONSTRUCT.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;You are not restricted to using the predicates and types that you are querying&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Effectively this means in CONSTRUCT you can build exactly the shape of RDF data that you want to process on the client. In my example, if I want to group the transactions together by the server that they are on, then the way that information is presented in the data is “backwards”. Sure I can query it backwards if I want to, but its more intuitive (to me at least) to restructure the results so that the server is the subject and the transaction log the object of the triples in my result graph.&lt;/p&gt;

&lt;p&gt;As the rest of the results are processed more triples get added to the result graph. And this is the second cool thing about CONSTRUCT.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Duplicates get merged&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So at the end of processing the results, my RDF graph looks like this (excuse the ASCII art):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;           log:processed
server:A +----------------&amp;gt; txn:132
         | log:processed
         +----------------&amp;gt; txn:135
         |
        ...
         
           log:processed
server:B +----------------&amp;gt; txn:130
         | log:processed
         +----------------&amp;gt; txn:133
         |
        ...

           log:processed
server:C +----------------&amp;gt; txn:134
         | log:processed
         +----------------&amp;gt; txn:131
         |
        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you are using Polaris to follow along here, you will find the results in RDF/XML on the &lt;code&gt;Results XML&lt;/code&gt; tab in the query window.&lt;/p&gt;

&lt;p&gt;When processing these results on the client we may have a little problem though. How do we know which servers are included in our results ? If we know beforehand the full list of servers we might expect in the results then we could just iterate that list and ask for the log:processed triples for each one, but that’s not going to work if we have no prior knowledge (or if someone
adds another server). What we need instead is some way to mark our “entry points” into the results graph. As this is just RDF, the answer is of course “add another triple”. One way to tackle this would be to mark the servers out by adding an rdf:type to them:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PREFIX log: &amp;lt;http://example.org/ont/transaction-log/&amp;gt;
CONSTRUCT {
    ?server a log:Server .
    ?server log:processed ?txn .
}
WHERE { 
    ?txn a log:Transaction . 
    ?txn log:processedBy ?server
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again we are using the ability to define types and predicates in our CONSTRUCT pattern that do not (necessarily) exist in the source data. We are also taking advantage of the fact that RDF automatically eliminates duplicate statements, so our results will only actually contain one &lt;code&gt;rdf:type&lt;/code&gt; statement for each server, and not 13 as you might think at first glance. The final RDF graph would be something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log:Server
    ^  rdf:type           log:processed
    |&amp;lt;-------  server:A +----------------&amp;gt; txn:132
    |                   | log:processed
    |                   +----------------&amp;gt; txn:135
    |                   |
    |                  ...
    |    
    |  rdf:type            log:processed
    +&amp;lt;--------- server:B +----------------&amp;gt; txn:130
    |                    | log:processed
    |                    +----------------&amp;gt; txn:133
    |                    |
    |                   ...
    |           
    |  rdf:type            log:processed
    +&amp;lt;--------- server:C +----------------&amp;gt; txn:134
                         | log:processed
                         +----------------&amp;gt; txn:131
                         |
                        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now our code to process this result can just look for &lt;code&gt;log:Server&lt;/code&gt; and find all resources connected to it by an rdf:type predicate to list out the servers, then follow the &lt;code&gt;log:processed&lt;/code&gt; predicate for each server to list out the transactions for that server.&lt;/p&gt;

&lt;p&gt;Hopefully what you can also see is that the graph view of results also makes it inherently far easier to work with queries that return items with lists of values for a given property. Because the SPARQL SELECT is restricted to a tabular result, you end up with one row for every possible combination of values which can lead to a massive explosion in the number of rows if you are dealing with complex data structures with several repeated properties. CONSTRUCT gets you out of that hole quite nicely because you are generating a graph from a graph rather than having to try and flatten everything out into a table.&lt;/p&gt;

&lt;p&gt;One issue that does come up with the use of CONSTRUCT however is that of sorting. The RDF graph created by a CONSTRUCT query is, just like any other RDF graph, an unordered collection of triples. So if you need your results in sorted order, you will need to do that on the client. So this is the third really cool thing about using CONSTRUCT.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;You can use SPARQL on the client-side to slice and dice results from the server&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Personally I find the easiest way to do this is to use a secondary SPARQL query on the client that queries just the RDF graph that is returned from the server. It may sound odd to do this, but in effect you are using the server to deliver you a chunk of data as an RDF graph and then the client to extract the final results from that chunk. Sometimes you will want to do the sorting on both the client and the server - a good example of this would be asking for ordered, paged results. As an example, lets say that we want the first 10 transaction logs sorted by date/time. On the server we would need a CONSTRUCT query like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PREFIX log: &amp;lt;http://example.org/ont/transaction-log/&amp;gt;
CONSTRUCT {
    ?server a log:Server .
    ?server log:processed ?txn .
	?txn log:processedAt ?procTime
}
WHERE { 
    ?txn a log:Transaction . 
    ?txn log:processedBy ?server .
	?txn log:processedAt ?procTime .
} ORDER BY ?procTime LIMIT 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The ORDER BY is needed on the server side because we want to ensure a consistent sort order when paging through the results. The LIMIT restricts us to the first 10 results. If you replace the &lt;code&gt;CONSTRUCT { ... }&lt;/code&gt; with &lt;code&gt;SELECT *&lt;/code&gt; and re-run the query you will see that the results table is sorted but because CONSTRUCT is generating a graph, that sorting gets lost. To reconstruct the correct order on the client we could just grovel through the RDF graph and do the sorting using standard programming language constructs; but if we have SPARQL capability on the client we can make life easy for ourselves by using a SELECT to pull out our results in the required order. On the client we might use a query like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PREFIX log: &amp;lt;http://example.org/ont/transaction-log/&amp;gt;
SELECT ?txn, ?time, ?server WHERE {
	?server log:processed ?txn .
	?txn log:processedAt ?time .
} ORDER BY ?time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: we don’t need the LIMIT here because we only pulled 10 results from the server.&lt;/p&gt;

&lt;p&gt;OK, for this simple example data using a CONSTRUCT in this way is just overhead; but if you imagine needing to retrieve more complex data where each transaction has one or more repeated properties then CONSTRUCT really is the way to go.&lt;/p&gt;

&lt;p&gt;This sort of pattern of pulling a graph from the server and querying it on the client can become really useful in a variety of circumstances. It allows you to hit the server up once for a bunch of data and then selectively extract slices of that data on the client side in response to user input or other UI requirements rather than having to constantly hit the server with lots of smaller queries; it also allows you to incrementally build up a local in-memory cache of data from the server - again with the potential to reduce the number of server round-trips you need to make.&lt;/p&gt;

&lt;p&gt;I hope this post has given you a bit of an insight into CONSTRUCT and has you hunting around for an RDF processing library for your chosen programming language! If you are on .NET, check out &lt;a href=&quot;http://dotnetrdf.org/&quot;&gt;DotNetRDF&lt;/a&gt;; Pythonistas should use &lt;a href=&quot;https://pypi.python.org/pypi/rdflib?&quot;&gt;rdflib&lt;/a&gt;; libraries are also available for many other languages that I don’t personally use - &lt;a href=&quot;https://duckduckgo.com/?q=rdf+library&quot;&gt;DuckDuckGo is your friend&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;If you have any thoughts or questions or would like to share your own cool CONSTRUCT tips, hit us up in the comments below!&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Oct 2015 14:41:38 +0100</pubDate>
        <link>http://networkedplanet.com/blog/2015/10/16/sparql-construct-101.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2015/10/16/sparql-construct-101.html</guid>
        
        <category>rdf</category>
        
        <category>sparql</category>
        
        <category>tutorial</category>
        
        
      </item>
    
      <item>
        <title>Open Data Camp 2</title>
        <description>&lt;p&gt;After missing out on ODCamp1 which was held in Winchester earlier in the year, I was looking forward to seeing what was in store for those attending the second. A &lt;a href=&quot;https://storify.com/RnROrganisation/odcamp-in-pics&quot;&gt;storify summary of ODCamp1&lt;/a&gt; had me excited for the content of the sessions that would be held, all with some kind of focus on Open Data, this time based in Manchester.&lt;/p&gt;

&lt;p&gt;ODCamp is an unconference - the venue, organisation and promotion are handled by the ODCamp team, and then the sessions are decided at the beginning of each day by any attendee lining up to quickly pitch what their session could entail. After each pitch a show of hands helps the organisers decide on what kind of space would be needed for the session - and very quickly the schedule is arranged for the day.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/odc_board_day1.jpg&quot; alt=&quot;Day 1 session schedule&quot; height=&quot;300px&quot; width=&quot;250px&quot; class=&quot;img-medium img-responsive center-block bordered-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Instantly I had a problem - I wanted to go to all of them! But short of waiting til nobody was looking and rearranging the post-it notes (an idea I quickly abandoned as too risky to escape unnoticed), I had to make some tough choices.&lt;/p&gt;

&lt;p&gt;Instead of attempting to summarise every session I went to (tempting, but this post would expand into thousands of words) just write about a few of my personal highlights (spoiler: I thought it was FANTASTIC) and urge you to &lt;a href=&quot;http://odcamp.org.uk/contact-us/&quot;&gt;stay up to date with the ODCamp team&lt;/a&gt; for news about future events.&lt;/p&gt;

&lt;h3 id=&quot;something-for-everyone&quot;&gt;Something for everyone&lt;/h3&gt;

&lt;p&gt;Attendees came from all kinds of different backgrounds, techies and non-techies alike. The one commonality was a shared interest in publishing data. The differences were a key element that made the camp work so well - each discussion would represent a fairly wide range of views and requirement needs, meaning that no matter what your own interest, you always walked away with food for thought.&lt;/p&gt;

&lt;h3 id=&quot;drawnalism&quot;&gt;Drawnalism&lt;/h3&gt;
&lt;p&gt;Matt from &lt;a href=&quot;http://drawnalism.com/&quot;&gt;Drawnalism&lt;/a&gt; was live recording sessions as large illustrations which were not only incredibly impressive to see happening and check out after a session, but I also knew just how valuable they would be after the event. It was seeing the illustrations from the last ODCamp that gave me such a clear idea of exactly what went on during the sessions, and after seeing them I’d instantly registered to not miss out on the next event. I know that sounds like a massive plug, but so be it, you cannot help but be impressed after turning around after an hour’s worth of lecture and/or dicussion to find the most salient points distilled and and then presented in a glorious hand illustrated info-graphic. Respect.&lt;/p&gt;

&lt;div style=&quot;max-width:820px&quot; class=&quot;center-block&quot;&gt;
	
&lt;p&gt;&lt;img src=&quot;/assets/images/blog/odc_dr_gov.jpg&quot; alt=&quot;Drawnalism illustration of the talk given by Director of Data from the Cabinet Office&quot; height=&quot;300px&quot; width=&quot;250px&quot; class=&quot;img-medium img-responsive bordered-image pull-left&quot; style=&quot;margin-right:20px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/odc_dr_youth.jpg&quot; alt=&quot;Drawnalism illustration of discussion around engaging youth with Open Data&quot; height=&quot;300px&quot; width=&quot;250px&quot; class=&quot;img-medium img-responsive bordered-image pull-left&quot; style=&quot;margin-right:20px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/odc_dr_defra.jpg&quot; alt=&quot;Drawnalism illustration of Open Data at DEFRA&quot; height=&quot;300px&quot; width=&quot;250px&quot; class=&quot;img-medium img-responsive bordered-image&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;making-open-data-suck-less&quot;&gt;Making Open Data Suck Less&lt;/h3&gt;

&lt;p&gt;With a provocative title for a pitch, Chris Gutteridge led a session towards the end of day one aimed at trying to improve the quality of Open Data. There were quite a few laughs to be had whilst people threw in their pet hates, and then very interesting discussions about what we could do to help alleviate them. Common problems were the publishing of “open” data in custom file formats, having to get through download forms to get to the data and bad data itself (e.g. a column of measurements that change whether it was recorded in mm or inches, sometimes using only numerals). As the session wrapped up it was great to hear solutions being thrashed out - such as suggestions for content of documentation (giving a better description of the data itself and links to learn more about it, what format is the data in? does it require a special program? is there a free version of the program if so?). Another good suggestion was a central wiki-style encyclopedia, an idea tempered by the need for that to be managed and someone’s responsibility. However, Amanda from &lt;a href=&quot;http://theodi.org/&quot;&gt;the ODI&lt;/a&gt; did mention that it was an idea that kept coming up and so I look forward to hearing more from them about it. &lt;/p&gt;

&lt;h3 id=&quot;unkeynotes&quot;&gt;Unkeynotes&lt;/h3&gt;

&lt;p&gt;Completely unplanned, there were two pitches that had such interest from everyone attending that the organisers helpfully did not place anyone else’s pitches at the same time. On day one it was bums on seats for a talk from Paul Maltby, Director of Data from the Cabinet Office. He gave an impassioned overview of the plans he and his team have to improve the way data is handled within the Cabinet Office. It will be interesting to keep track of the progress they make over the next year or so and whether the good intentions turn into published datasets.&lt;/p&gt;

&lt;p&gt;John Murray’s session on spatial analysis queries led into a interesting discussions on personal perception of “place”, and on to who really “owns” addresses. As time ran out, this prompted a pitch for more on the debate - turning into day two’s unkeynote from Bob Barr with the full rundown “Address Wars” history. I can’t possibly summarise it all here, but we can only hope that &lt;a href=&quot;https://alpha.openaddressesuk.org/blog/2015/07/27/a-time-for-going-to-bed&quot;&gt;Open Addresses gets another boost to continue the amazing work they started&lt;/a&gt;. &lt;/p&gt;

&lt;h3 id=&quot;its-important-to-demystify-rdf&quot;&gt;It’s important to demystify RDF&lt;/h3&gt;

&lt;p&gt;For an event targeted specifically at Open Data enthusiasts, I was surprised to find that there were very few that were comfortable with Linked Data. Some were unaware of it entirely, some knew a bit about it but weren’t sure of its benefits, and others were very comfortable with the idea of it but put it to one side as if it was some kind of pipe-dream rather than a simple step onwards from CSV style tabular data. It prompted me to do a pitch myself on the second day, targeted specifically to those people who would like to have a play around with RDF, but weren’t sure how to go about doing that. I ran a quick session to use OpenRefine to convert CSV to RDF using the RDF extension to map columns to predicates, and then we had a bit of a chat around the subjects of re-using existing Linked Data vocabularies (do you &lt;em&gt;have&lt;/em&gt; to? no you don’t). Fear not if you’d like me to write more on this subject, I’ve taken notes from the questions asked during that session and will do a separate blog post on that subject.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/odc_dr_learn.jpg&quot; alt=&quot;Drawnalism illustration zoom in: &amp;quot;There&#39;s a massive process of internal learning going on (and it must continue)&amp;quot;&quot; height=&quot;300px&quot; width=&quot;250px&quot; class=&quot;img-medium img-responsive center-block bordered-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The weekend-long camp concluded with a feedback session to chat about any points that particularly worked or didn’t work over the weekend. This was in keeping with the unconference ethos - the attendees come together and pitch in to build a conference on the fly. It works fantastically when done well, and hats off to the organisers for making ODCamp one of those. ODCamp3 is looking like it will be held in about six months time - &lt;a href=&quot;http://odcamp.org.uk/contact-us/&quot;&gt;head over to ODCamp website and sign up to get updates&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;also-check-out&quot;&gt;Also check out:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.flickr.com/photos/puntofisso/albums/72157659650939562&quot;&gt;Open Data Camp 2 in pictures&lt;/a&gt; (via Guiseppe Sollazzo on Flickr)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/hashtag/odcamp?vertical=default&amp;amp;src=hash&quot;&gt;The #odcamp hashtag on twitter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://odcamp.org.uk/&quot;&gt;ODCamp website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 13 Oct 2015 14:17:38 +0100</pubDate>
        <link>http://networkedplanet.com/blog/2015/10/13/open-data-camp.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2015/10/13/open-data-camp.html</guid>
        
        <category>linked data</category>
        
        <category>open data</category>
        
        <category>opinion</category>
        
        <category>conference</category>
        
        
      </item>
    
      <item>
        <title>Linked Data Webinar  15/10</title>
        <description>&lt;p&gt;We are pleased to announce our second training webinar, “Introduction to Linked Data”. This is a (mostly)
non-technical presentation focussed on the what and why of Linked Data - what is Linked Data, what are 
the benefits of publishing and using Linked Data, what are the steps to take in publishing your own
Linked Data either internally or externally.&lt;/p&gt;

&lt;p&gt;The webinar will be presented by Kal Ahmed who in addition to being the founder of NetworkedPlanet is also
the lead developer of &lt;a href=&quot;http://brightstardb.com/&quot;&gt;BrightstarDB&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This webinar is FREE, but places are limited.&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;btn btn-primary btn-lg&quot; href=&quot;https://www.eventbrite.co.uk/e/webinar-introduction-to-linked-data-registration-18733197498&quot;&gt;Sign Up Here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We plan to follow this webinar up with others looking at SPARQL; RDF; BrightstarDB;
and the &lt;a href=&quot;http://dataplatform.co.uk/&quot;&gt;Data Platform&lt;/a&gt;. If you would like to get a heads-up on these
future webinars when they are announced, &lt;a href=&quot;https://twitter.com/nwplanet&quot;&gt;follow us on Twitter&lt;/a&gt; or &lt;a href=&quot;/#contact&quot;&gt;sign-up for the NetworkedPlanet Newsletter&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 12 Oct 2015 00:00:00 +0100</pubDate>
        <link>http://networkedplanet.com/blog/2015/10/12/linked-data-webinar-oct-15.html</link>
        <guid isPermaLink="true">http://networkedplanet.com/blog/2015/10/12/linked-data-webinar-oct-15.html</guid>
        
        <category>webinar</category>
        
        
      </item>
    
  </channel>
</rss>
